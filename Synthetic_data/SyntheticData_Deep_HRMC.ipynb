{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var 0: <tf.Variable 'flattened_layer_1/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 1: <tf.Variable 'flattened_layer_1/bias:0' shape=(200,) dtype=float32>\n",
      "Var 2: <tf.Variable 'flattened_layer_2/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 3: <tf.Variable 'flattened_layer_2/bias:0' shape=(200,) dtype=float32>\n",
      "Var 4: <tf.Variable 'flattened_layer_3/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 5: <tf.Variable 'flattened_layer_3/bias:0' shape=(200,) dtype=float32>\n",
      "Var 6: <tf.Variable 'flattened_layer_4/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 7: <tf.Variable 'flattened_layer_4/bias:0' shape=(200,) dtype=float32>\n",
      "Var 8: <tf.Variable 'flattened_layer_5/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 9: <tf.Variable 'flattened_layer_5/bias:0' shape=(200,) dtype=float32>\n",
      "Var 10: <tf.Variable 'flattened_layer_6/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 11: <tf.Variable 'flattened_layer_6/bias:0' shape=(200,) dtype=float32>\n",
      "Var 12: <tf.Variable 'flattened_layer_7/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 13: <tf.Variable 'flattened_layer_7/bias:0' shape=(200,) dtype=float32>\n",
      "Var 14: <tf.Variable 'flattened_layer_8/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 15: <tf.Variable 'flattened_layer_8/bias:0' shape=(200,) dtype=float32>\n",
      "Var 16: <tf.Variable 'flattened_layer_9/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 17: <tf.Variable 'flattened_layer_9/bias:0' shape=(200,) dtype=float32>\n",
      "Var 18: <tf.Variable 'flattened_layer_10/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 19: <tf.Variable 'flattened_layer_10/bias:0' shape=(200,) dtype=float32>\n",
      "Var 20: <tf.Variable 'flattened_layer_11/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 21: <tf.Variable 'flattened_layer_11/bias:0' shape=(200,) dtype=float32>\n",
      "Var 22: <tf.Variable 'flattened_layer_12/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 23: <tf.Variable 'flattened_layer_12/bias:0' shape=(200,) dtype=float32>\n",
      "Var 24: <tf.Variable 'flattened_layer_13/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 25: <tf.Variable 'flattened_layer_13/bias:0' shape=(200,) dtype=float32>\n",
      "Var 26: <tf.Variable 'flattened_layer_14/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 27: <tf.Variable 'flattened_layer_14/bias:0' shape=(200,) dtype=float32>\n",
      "Var 28: <tf.Variable 'flattened_layer_15/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 29: <tf.Variable 'flattened_layer_15/bias:0' shape=(200,) dtype=float32>\n",
      "Var 30: <tf.Variable 'flattened_layer_16/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 31: <tf.Variable 'flattened_layer_16/bias:0' shape=(200,) dtype=float32>\n",
      "Var 32: <tf.Variable 'flattened_layer_17/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 33: <tf.Variable 'flattened_layer_17/bias:0' shape=(200,) dtype=float32>\n",
      "Var 34: <tf.Variable 'flattened_layer_18/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 35: <tf.Variable 'flattened_layer_18/bias:0' shape=(200,) dtype=float32>\n",
      "Var 36: <tf.Variable 'flattened_layer_19/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 37: <tf.Variable 'flattened_layer_19/bias:0' shape=(200,) dtype=float32>\n",
      "Var 38: <tf.Variable 'flattened_layer_20/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 39: <tf.Variable 'flattened_layer_20/bias:0' shape=(200,) dtype=float32>\n",
      "Var 40: <tf.Variable 'flattened_layer_21/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 41: <tf.Variable 'flattened_layer_21/bias:0' shape=(200,) dtype=float32>\n",
      "Var 42: <tf.Variable 'flattened_layer_22/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 43: <tf.Variable 'flattened_layer_22/bias:0' shape=(200,) dtype=float32>\n",
      "Var 44: <tf.Variable 'flattened_layer_23/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 45: <tf.Variable 'flattened_layer_23/bias:0' shape=(200,) dtype=float32>\n",
      "Var 46: <tf.Variable 'flattened_layer_24/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 47: <tf.Variable 'flattened_layer_24/bias:0' shape=(200,) dtype=float32>\n",
      "Var 48: <tf.Variable 'flattened_layer_25/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 49: <tf.Variable 'flattened_layer_25/bias:0' shape=(200,) dtype=float32>\n",
      "Var 50: <tf.Variable 'flattened_layer_26/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 51: <tf.Variable 'flattened_layer_26/bias:0' shape=(200,) dtype=float32>\n",
      "Var 52: <tf.Variable 'flattened_layer_27/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 53: <tf.Variable 'flattened_layer_27/bias:0' shape=(200,) dtype=float32>\n",
      "Var 54: <tf.Variable 'flattened_layer_28/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 55: <tf.Variable 'flattened_layer_28/bias:0' shape=(200,) dtype=float32>\n",
      "Var 56: <tf.Variable 'flattened_layer_29/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 57: <tf.Variable 'flattened_layer_29/bias:0' shape=(200,) dtype=float32>\n",
      "Var 58: <tf.Variable 'flattened_layer_30/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 59: <tf.Variable 'flattened_layer_30/bias:0' shape=(200,) dtype=float32>\n",
      "Var 60: <tf.Variable 'flattened_layer_31/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 61: <tf.Variable 'flattened_layer_31/bias:0' shape=(200,) dtype=float32>\n",
      "Var 62: <tf.Variable 'flattened_layer_32/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 63: <tf.Variable 'flattened_layer_32/bias:0' shape=(200,) dtype=float32>\n",
      "Var 64: <tf.Variable 'flattened_layer_33/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 65: <tf.Variable 'flattened_layer_33/bias:0' shape=(200,) dtype=float32>\n",
      "Var 66: <tf.Variable 'flattened_layer_34/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 67: <tf.Variable 'flattened_layer_34/bias:0' shape=(200,) dtype=float32>\n",
      "Var 68: <tf.Variable 'flattened_layer_35/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 69: <tf.Variable 'flattened_layer_35/bias:0' shape=(200,) dtype=float32>\n",
      "Var 70: <tf.Variable 'flattened_layer_36/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 71: <tf.Variable 'flattened_layer_36/bias:0' shape=(200,) dtype=float32>\n",
      "Var 72: <tf.Variable 'flattened_layer_37/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 73: <tf.Variable 'flattened_layer_37/bias:0' shape=(200,) dtype=float32>\n",
      "Var 74: <tf.Variable 'flattened_layer_38/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 75: <tf.Variable 'flattened_layer_38/bias:0' shape=(200,) dtype=float32>\n",
      "Var 76: <tf.Variable 'flattened_layer_39/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 77: <tf.Variable 'flattened_layer_39/bias:0' shape=(200,) dtype=float32>\n",
      "Var 78: <tf.Variable 'flattened_layer_40/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 79: <tf.Variable 'flattened_layer_40/bias:0' shape=(200,) dtype=float32>\n",
      "Var 80: <tf.Variable 'flattened_layer_41/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 81: <tf.Variable 'flattened_layer_41/bias:0' shape=(200,) dtype=float32>\n",
      "Var 82: <tf.Variable 'flattened_layer_42/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 83: <tf.Variable 'flattened_layer_42/bias:0' shape=(200,) dtype=float32>\n",
      "Var 84: <tf.Variable 'flattened_layer_43/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 85: <tf.Variable 'flattened_layer_43/bias:0' shape=(200,) dtype=float32>\n",
      "Var 86: <tf.Variable 'flattened_layer_44/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 87: <tf.Variable 'flattened_layer_44/bias:0' shape=(200,) dtype=float32>\n",
      "Var 88: <tf.Variable 'flattened_layer_45/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 89: <tf.Variable 'flattened_layer_45/bias:0' shape=(200,) dtype=float32>\n",
      "Var 90: <tf.Variable 'flattened_layer_46/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 91: <tf.Variable 'flattened_layer_46/bias:0' shape=(200,) dtype=float32>\n",
      "Var 92: <tf.Variable 'flattened_layer_47/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 93: <tf.Variable 'flattened_layer_47/bias:0' shape=(200,) dtype=float32>\n",
      "Var 94: <tf.Variable 'flattened_layer_48/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 95: <tf.Variable 'flattened_layer_48/bias:0' shape=(200,) dtype=float32>\n",
      "Var 96: <tf.Variable 'flattened_layer_49/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 97: <tf.Variable 'flattened_layer_49/bias:0' shape=(200,) dtype=float32>\n",
      "Var 98: <tf.Variable 'flattened_layer_50/kernel:0' shape=(200, 200) dtype=float32>\n",
      "Var 99: <tf.Variable 'flattened_layer_50/bias:0' shape=(200,) dtype=float32>\n",
      "Var 100: <tf.Variable 'encoder_layer_1/kernel:0' shape=(50, 40) dtype=float32>\n",
      "Var 101: <tf.Variable 'encoder_layer_1/bias:0' shape=(40,) dtype=float32>\n",
      "Var 102: <tf.Variable 'Self-Expressive_Layer/Coef:0' shape=(200, 200) dtype=float32>\n",
      "Var 103: <tf.Variable 'decoder_layer_1/kernel:0' shape=(40, 50) dtype=float32>\n",
      "Var 104: <tf.Variable 'decoder_layer_1/bias:0' shape=(50,) dtype=float32>\n",
      "epoch: 1 :: cost: 0.48656470 :: accuracy: 0.99823177 \n",
      "epoch: 2 :: cost: 0.41308773 :: accuracy: 0.84857321 \n",
      "epoch: 3 :: cost: 0.37276408 :: accuracy: 0.76526386 \n",
      "epoch: 4 :: cost: 0.38023016 :: accuracy: 0.78062564 \n",
      "epoch: 5 :: cost: 0.32560197 :: accuracy: 0.66854185 \n",
      "epoch: 6 :: cost: 0.32205167 :: accuracy: 0.66096514 \n",
      "epoch: 7 :: cost: 0.31882399 :: accuracy: 0.65401226 \n",
      "epoch: 8 :: cost: 0.31947342 :: accuracy: 0.65521282 \n",
      "epoch: 9 :: cost: 0.31149930 :: accuracy: 0.63934708 \n",
      "epoch: 10 :: cost: 0.29990172 :: accuracy: 0.61627096 \n",
      "epoch: 11 :: cost: 0.28394946 :: accuracy: 0.58438277 \n",
      "epoch: 12 :: cost: 0.26456615 :: accuracy: 0.54595840 \n",
      "epoch: 13 :: cost: 0.24366061 :: accuracy: 0.50422078 \n",
      "epoch: 14 :: cost: 0.21832022 :: accuracy: 0.45313132 \n",
      "epoch: 15 :: cost: 0.20631462 :: accuracy: 0.42863357 \n",
      "epoch: 16 :: cost: 0.19643182 :: accuracy: 0.40839940 \n",
      "epoch: 17 :: cost: 0.17774525 :: accuracy: 0.37026292 \n",
      "epoch: 18 :: cost: 0.15519348 :: accuracy: 0.32476932 \n",
      "epoch: 19 :: cost: 0.14217754 :: accuracy: 0.29872531 \n",
      "epoch: 20 :: cost: 0.13408206 :: accuracy: 0.28198665 \n",
      "epoch: 21 :: cost: 0.12816770 :: accuracy: 0.26954973 \n",
      "epoch: 22 :: cost: 0.12225534 :: accuracy: 0.25769410 \n",
      "epoch: 23 :: cost: 0.11705460 :: accuracy: 0.24760379 \n",
      "epoch: 24 :: cost: 0.11179864 :: accuracy: 0.23723224 \n",
      "epoch: 25 :: cost: 0.10447828 :: accuracy: 0.22233473 \n",
      "epoch: 26 :: cost: 0.09818654 :: accuracy: 0.20966579 \n",
      "epoch: 27 :: cost: 0.09368072 :: accuracy: 0.20025200 \n",
      "epoch: 28 :: cost: 0.09144137 :: accuracy: 0.19530080 \n",
      "epoch: 29 :: cost: 0.09847871 :: accuracy: 0.20879541 \n",
      "epoch: 30 :: cost: 0.10705216 :: accuracy: 0.22538598 \n",
      "epoch: 31 :: cost: 0.10189722 :: accuracy: 0.21529856 \n",
      "epoch: 32 :: cost: 0.09026136 :: accuracy: 0.19168490 \n",
      "epoch: 33 :: cost: 0.09483244 :: accuracy: 0.20026605 \n",
      "epoch: 34 :: cost: 0.09066364 :: accuracy: 0.19260998 \n",
      "epoch: 35 :: cost: 0.08102885 :: accuracy: 0.17319341 \n",
      "epoch: 36 :: cost: 0.09116992 :: accuracy: 0.19231756 \n",
      "epoch: 37 :: cost: 0.07546777 :: accuracy: 0.16150850 \n",
      "epoch: 38 :: cost: 0.08195997 :: accuracy: 0.17447147 \n",
      "epoch: 39 :: cost: 0.07423605 :: accuracy: 0.15860181 \n",
      "epoch: 40 :: cost: 0.07381705 :: accuracy: 0.15738450 \n",
      "epoch: 41 :: cost: 0.07242873 :: accuracy: 0.15465491 \n",
      "epoch: 42 :: cost: 0.06618044 :: accuracy: 0.14206484 \n",
      "epoch: 43 :: cost: 0.06579868 :: accuracy: 0.14054358 \n",
      "epoch: 44 :: cost: 0.06288008 :: accuracy: 0.13493648 \n",
      "epoch: 45 :: cost: 0.05644077 :: accuracy: 0.12203583 \n",
      "epoch: 46 :: cost: 0.05832763 :: accuracy: 0.12534544 \n",
      "epoch: 47 :: cost: 0.05613159 :: accuracy: 0.12072681 \n",
      "epoch: 48 :: cost: 0.04619664 :: accuracy: 0.10113557 \n",
      "epoch: 49 :: cost: 0.04881554 :: accuracy: 0.10631164 \n",
      "epoch: 50 :: cost: 0.04940517 :: accuracy: 0.10737435 \n",
      "epoch: 51 :: cost: 0.04223815 :: accuracy: 0.09259880 \n",
      "epoch: 52 :: cost: 0.03250281 :: accuracy: 0.07450972 \n",
      "epoch: 53 :: cost: 0.03745863 :: accuracy: 0.08394296 \n",
      "epoch: 54 :: cost: 0.03165768 :: accuracy: 0.07277952 \n",
      "epoch: 55 :: cost: 0.03149500 :: accuracy: 0.07190533 \n",
      "epoch: 56 :: cost: 0.03038309 :: accuracy: 0.06965306 \n",
      "epoch: 57 :: cost: 0.02607121 :: accuracy: 0.06145677 \n",
      "epoch: 58 :: cost: 0.02859472 :: accuracy: 0.06560975 \n",
      "epoch: 59 :: cost: 0.02677992 :: accuracy: 0.06247403 \n",
      "epoch: 60 :: cost: 0.02288298 :: accuracy: 0.05489882 \n",
      "epoch: 61 :: cost: 0.02333860 :: accuracy: 0.05513641 \n",
      "epoch: 62 :: cost: 0.02575191 :: accuracy: 0.05939601 \n",
      "epoch: 63 :: cost: 0.01958604 :: accuracy: 0.04751994 \n",
      "epoch: 64 :: cost: 0.01780132 :: accuracy: 0.04437503 \n",
      "epoch: 65 :: cost: 0.01646274 :: accuracy: 0.04182111 \n",
      "epoch: 66 :: cost: 0.01522565 :: accuracy: 0.03892214 \n",
      "epoch: 67 :: cost: 0.01996075 :: accuracy: 0.04701517 \n",
      "epoch: 68 :: cost: 0.02455942 :: accuracy: 0.05449466 \n",
      "epoch: 69 :: cost: 0.01789724 :: accuracy: 0.04249819 \n",
      "epoch: 70 :: cost: 0.01389846 :: accuracy: 0.03457991 \n",
      "epoch: 71 :: cost: 0.01762754 :: accuracy: 0.04141480 \n",
      "epoch: 72 :: cost: 0.02141015 :: accuracy: 0.04735313 \n",
      "epoch: 73 :: cost: 0.01709458 :: accuracy: 0.03965235 \n",
      "epoch: 74 :: cost: 0.01249638 :: accuracy: 0.03065028 \n",
      "epoch: 75 :: cost: 0.01776793 :: accuracy: 0.04042434 \n",
      "epoch: 76 :: cost: 0.01882161 :: accuracy: 0.04135698 \n",
      "epoch: 77 :: cost: 0.01419683 :: accuracy: 0.03313186 \n",
      "epoch: 78 :: cost: 0.01379444 :: accuracy: 0.03171133 \n",
      "epoch: 79 :: cost: 0.01936605 :: accuracy: 0.04266854 \n",
      "epoch: 80 :: cost: 0.01148134 :: accuracy: 0.02713853 \n",
      "epoch: 81 :: cost: 0.01313048 :: accuracy: 0.03047660 \n",
      "epoch: 82 :: cost: 0.01884205 :: accuracy: 0.04052658 \n",
      "epoch: 83 :: cost: 0.01306211 :: accuracy: 0.02995136 \n",
      "epoch: 84 :: cost: 0.01091264 :: accuracy: 0.02541704 \n",
      "epoch: 85 :: cost: 0.01920293 :: accuracy: 0.04152599 \n",
      "epoch: 86 :: cost: 0.00893983 :: accuracy: 0.02161821 \n",
      "epoch: 87 :: cost: 0.01293600 :: accuracy: 0.02918326 \n",
      "epoch: 88 :: cost: 0.01448914 :: accuracy: 0.03153716 \n",
      "epoch: 89 :: cost: 0.01642015 :: accuracy: 0.03571038 \n",
      "epoch: 90 :: cost: 0.00927519 :: accuracy: 0.02177638 \n",
      "epoch: 91 :: cost: 0.01109023 :: accuracy: 0.02520835 \n",
      "epoch: 92 :: cost: 0.01263993 :: accuracy: 0.02773325 \n",
      "epoch: 93 :: cost: 0.01804700 :: accuracy: 0.03863352 \n",
      "epoch: 94 :: cost: 0.00950567 :: accuracy: 0.02186978 \n",
      "epoch: 95 :: cost: 0.01145324 :: accuracy: 0.02541667 \n",
      "epoch: 96 :: cost: 0.01354374 :: accuracy: 0.02958036 \n",
      "epoch: 97 :: cost: 0.01682647 :: accuracy: 0.03559164 \n",
      "epoch: 98 :: cost: 0.01115592 :: accuracy: 0.02493011 \n",
      "epoch: 99 :: cost: 0.00627551 :: accuracy: 0.01599636 \n",
      "epoch: 100 :: cost: 0.01216901 :: accuracy: 0.02655402 \n",
      "epoch: 101 :: cost: 0.01192659 :: accuracy: 0.02589138 \n",
      "epoch: 102 :: cost: 0.00695212 :: accuracy: 0.01704911 \n",
      "epoch: 103 :: cost: 0.00621499 :: accuracy: 0.01566518 \n",
      "epoch: 104 :: cost: 0.00703562 :: accuracy: 0.01678827 \n",
      "epoch: 105 :: cost: 0.01040696 :: accuracy: 0.02327493 \n",
      "epoch: 106 :: cost: 0.00607479 :: accuracy: 0.01531523 \n",
      "epoch: 107 :: cost: 0.00925042 :: accuracy: 0.02074655 \n",
      "epoch: 108 :: cost: 0.00976451 :: accuracy: 0.02194184 \n",
      "epoch: 109 :: cost: 0.00539704 :: accuracy: 0.01405778 \n",
      "epoch: 110 :: cost: 0.01280480 :: accuracy: 0.02743650 \n",
      "epoch: 111 :: cost: 0.00756931 :: accuracy: 0.01770164 \n",
      "epoch: 112 :: cost: 0.01115282 :: accuracy: 0.02452092 \n",
      "epoch: 113 :: cost: 0.00746642 :: accuracy: 0.01730015 \n",
      "epoch: 114 :: cost: 0.00609392 :: accuracy: 0.01491864 \n",
      "epoch: 115 :: cost: 0.01303104 :: accuracy: 0.02813487 \n",
      "epoch: 116 :: cost: 0.00848579 :: accuracy: 0.01926870 \n",
      "epoch: 117 :: cost: 0.01518472 :: accuracy: 0.03198817 \n",
      "epoch: 118 :: cost: 0.00867284 :: accuracy: 0.01934869 \n",
      "epoch: 119 :: cost: 0.01965870 :: accuracy: 0.04127520 \n",
      "epoch: 120 :: cost: 0.01970723 :: accuracy: 0.04136887 \n",
      "epoch: 121 :: cost: 0.00711608 :: accuracy: 0.01650230 \n",
      "epoch: 122 :: cost: 0.01232796 :: accuracy: 0.02630712 \n",
      "epoch: 123 :: cost: 0.00901857 :: accuracy: 0.02017135 \n",
      "epoch: 124 :: cost: 0.00831455 :: accuracy: 0.01885758 \n",
      "epoch: 125 :: cost: 0.01227532 :: accuracy: 0.02613457 \n",
      "epoch: 126 :: cost: 0.00783529 :: accuracy: 0.01762271 \n",
      "epoch: 127 :: cost: 0.01614611 :: accuracy: 0.03415198 \n",
      "epoch: 128 :: cost: 0.01399660 :: accuracy: 0.02984489 \n",
      "epoch: 129 :: cost: 0.01232907 :: accuracy: 0.02625118 \n",
      "epoch: 130 :: cost: 0.01103803 :: accuracy: 0.02369990 \n",
      "epoch: 131 :: cost: 0.01356849 :: accuracy: 0.02893837 \n",
      "epoch: 132 :: cost: 0.01276205 :: accuracy: 0.02732549 \n",
      "epoch: 133 :: cost: 0.01068358 :: accuracy: 0.02290843 \n",
      "epoch: 134 :: cost: 0.00858979 :: accuracy: 0.01891260 \n",
      "epoch: 135 :: cost: 0.01509000 :: accuracy: 0.03195461 \n",
      "epoch: 136 :: cost: 0.01408981 :: accuracy: 0.02995187 \n",
      "epoch: 137 :: cost: 0.00974025 :: accuracy: 0.02111520 \n",
      "epoch: 138 :: cost: 0.00838596 :: accuracy: 0.01848474 \n",
      "epoch: 139 :: cost: 0.01455068 :: accuracy: 0.03084657 \n",
      "epoch: 140 :: cost: 0.01309697 :: accuracy: 0.02793277 \n",
      "epoch: 141 :: cost: 0.01029417 :: accuracy: 0.02210526 \n",
      "epoch: 142 :: cost: 0.00862670 :: accuracy: 0.01891861 \n",
      "epoch: 143 :: cost: 0.01468355 :: accuracy: 0.03109550 \n",
      "epoch: 144 :: cost: 0.01421879 :: accuracy: 0.03015650 \n",
      "epoch: 145 :: cost: 0.00796605 :: accuracy: 0.01759683 \n",
      "epoch: 146 :: cost: 0.00650025 :: accuracy: 0.01487892 \n",
      "epoch: 147 :: cost: 0.01522442 :: accuracy: 0.03215211 \n",
      "epoch: 148 :: cost: 0.01325372 :: accuracy: 0.02819996 \n",
      "epoch: 149 :: cost: 0.00989981 :: accuracy: 0.02127210 \n",
      "epoch: 150 :: cost: 0.00876670 :: accuracy: 0.01904920 \n",
      "epoch: 151 :: cost: 0.01369899 :: accuracy: 0.02906748 \n",
      "epoch: 152 :: cost: 0.01335428 :: accuracy: 0.02838940 \n",
      "epoch: 153 :: cost: 0.00280185 :: accuracy: 0.00898258 \n",
      "epoch: 154 :: cost: 0.01609378 :: accuracy: 0.03352447 \n",
      "epoch: 155 :: cost: 0.02046586 :: accuracy: 0.04234738 \n",
      "epoch: 156 :: cost: 0.01301380 :: accuracy: 0.02735639 \n",
      "epoch: 157 :: cost: 0.00787345 :: accuracy: 0.01752961 \n",
      "epoch: 158 :: cost: 0.01315456 :: accuracy: 0.02792915 \n",
      "epoch: 159 :: cost: 0.00929290 :: accuracy: 0.02032079 \n",
      "epoch: 160 :: cost: 0.00810488 :: accuracy: 0.01776209 \n",
      "epoch: 161 :: cost: 0.01045989 :: accuracy: 0.02229640 \n",
      "epoch: 162 :: cost: 0.00460005 :: accuracy: 0.01146518 \n",
      "epoch: 163 :: cost: 0.01040719 :: accuracy: 0.02246025 \n",
      "epoch: 164 :: cost: 0.01093870 :: accuracy: 0.02347635 \n",
      "epoch: 165 :: cost: 0.00488438 :: accuracy: 0.01189235 \n",
      "epoch: 166 :: cost: 0.00947382 :: accuracy: 0.02032750 \n",
      "epoch: 167 :: cost: 0.00755922 :: accuracy: 0.01668636 \n",
      "epoch: 168 :: cost: 0.00737210 :: accuracy: 0.01658117 \n",
      "epoch: 169 :: cost: 0.00837939 :: accuracy: 0.01850100 \n",
      "epoch: 170 :: cost: 0.00384645 :: accuracy: 0.01013349 \n",
      "epoch: 171 :: cost: 0.00658413 :: accuracy: 0.01479004 \n",
      "epoch: 172 :: cost: 0.00443539 :: accuracy: 0.01107597 \n",
      "epoch: 173 :: cost: 0.00620011 :: accuracy: 0.01434841 \n",
      "epoch: 174 :: cost: 0.00348001 :: accuracy: 0.00960346 \n",
      "epoch: 175 :: cost: 0.00674102 :: accuracy: 0.01505207 \n",
      "epoch: 176 :: cost: 0.00456916 :: accuracy: 0.01118879 \n",
      "epoch: 177 :: cost: 0.00702176 :: accuracy: 0.01583543 \n",
      "epoch: 178 :: cost: 0.00547474 :: accuracy: 0.01298423 \n",
      "epoch: 179 :: cost: 0.00704630 :: accuracy: 0.01561395 \n",
      "epoch: 180 :: cost: 0.00610983 :: accuracy: 0.01383933 \n",
      "epoch: 181 :: cost: 0.00635975 :: accuracy: 0.01450825 \n",
      "epoch: 182 :: cost: 0.00606298 :: accuracy: 0.01397970 \n",
      "epoch: 183 :: cost: 0.00538939 :: accuracy: 0.01255449 \n",
      "epoch: 184 :: cost: 0.00478359 :: accuracy: 0.01146647 \n",
      "epoch: 185 :: cost: 0.00636938 :: accuracy: 0.01451809 \n",
      "epoch: 186 :: cost: 0.00579741 :: accuracy: 0.01342805 \n",
      "epoch: 187 :: cost: 0.00552470 :: accuracy: 0.01270476 \n",
      "epoch: 188 :: cost: 0.00457602 :: accuracy: 0.01105756 \n",
      "epoch: 189 :: cost: 0.00694847 :: accuracy: 0.01558585 \n",
      "epoch: 190 :: cost: 0.00616164 :: accuracy: 0.01407932 \n",
      "epoch: 191 :: cost: 0.00566784 :: accuracy: 0.01294077 \n",
      "epoch: 192 :: cost: 0.00505300 :: accuracy: 0.01182516 \n",
      "epoch: 193 :: cost: 0.00645912 :: accuracy: 0.01463085 \n",
      "epoch: 194 :: cost: 0.00606585 :: accuracy: 0.01387942 \n",
      "epoch: 195 :: cost: 0.00497073 :: accuracy: 0.01163401 \n",
      "epoch: 196 :: cost: 0.00408855 :: accuracy: 0.01011809 \n",
      "epoch: 197 :: cost: 0.00728108 :: accuracy: 0.01615864 \n",
      "epoch: 198 :: cost: 0.00682272 :: accuracy: 0.01527929 \n",
      "epoch: 199 :: cost: 0.00446439 :: accuracy: 0.01071625 \n",
      "epoch: 200 :: cost: 0.00391443 :: accuracy: 0.00975487 \n",
      "epoch: 201 :: cost: 0.00703679 :: accuracy: 0.01564537 \n",
      "epoch: 202 :: cost: 0.00648207 :: accuracy: 0.01460017 \n",
      "epoch: 203 :: cost: 0.00128018 :: accuracy: 0.00633343 \n",
      "epoch: 204 :: cost: 0.00646534 :: accuracy: 0.01429767 \n",
      "epoch: 205 :: cost: 0.00669043 :: accuracy: 0.01470633 \n",
      "epoch: 206 :: cost: 0.00232905 :: accuracy: 0.00732441 \n",
      "epoch: 207 :: cost: 0.00624733 :: accuracy: 0.01413534 \n",
      "epoch: 208 :: cost: 0.00704645 :: accuracy: 0.01568365 \n",
      "epoch: 209 :: cost: 0.00347958 :: accuracy: 0.00925408 \n",
      "epoch: 210 :: cost: 0.00573429 :: accuracy: 0.01293607 \n",
      "epoch: 211 :: cost: 0.00671476 :: accuracy: 0.01473100 \n",
      "epoch: 212 :: cost: 0.00357731 :: accuracy: 0.00912036 \n",
      "epoch: 213 :: cost: 0.00536612 :: accuracy: 0.01242865 \n",
      "epoch: 214 :: cost: 0.00599422 :: accuracy: 0.01363214 \n",
      "epoch: 215 :: cost: 0.00358110 :: accuracy: 0.00931961 \n",
      "epoch: 216 :: cost: 0.00490172 :: accuracy: 0.01141082 \n",
      "epoch: 217 :: cost: 0.00445845 :: accuracy: 0.01057276 \n",
      "epoch: 218 :: cost: 0.00377193 :: accuracy: 0.00949571 \n",
      "epoch: 219 :: cost: 0.00420061 :: accuracy: 0.01030902 \n",
      "epoch: 220 :: cost: 0.00266196 :: accuracy: 0.00779836 \n",
      "epoch: 221 :: cost: 0.00390939 :: accuracy: 0.00964935 \n",
      "epoch: 222 :: cost: 0.00227622 :: accuracy: 0.00713635 \n",
      "epoch: 223 :: cost: 0.00452206 :: accuracy: 0.01082848 \n",
      "epoch: 224 :: cost: 0.00420161 :: accuracy: 0.01026659 \n",
      "epoch: 225 :: cost: 0.00226892 :: accuracy: 0.00708849 \n",
      "epoch: 226 :: cost: 0.00268240 :: accuracy: 0.00765299 \n",
      "epoch: 227 :: cost: 0.00267679 :: accuracy: 0.00776956 \n",
      "epoch: 228 :: cost: 0.00201300 :: accuracy: 0.00683166 \n",
      "epoch: 229 :: cost: 0.00379154 :: accuracy: 0.00935365 \n",
      "epoch: 230 :: cost: 0.00319761 :: accuracy: 0.00838210 \n",
      "epoch: 231 :: cost: 0.00317718 :: accuracy: 0.00849905 \n",
      "epoch: 232 :: cost: 0.00304207 :: accuracy: 0.00827085 \n",
      "epoch: 233 :: cost: 0.00300461 :: accuracy: 0.00806649 \n",
      "epoch: 234 :: cost: 0.00250492 :: accuracy: 0.00731192 \n",
      "epoch: 235 :: cost: 0.00362749 :: accuracy: 0.00919857 \n",
      "epoch: 236 :: cost: 0.00337331 :: accuracy: 0.00875479 \n",
      "epoch: 237 :: cost: 0.00258846 :: accuracy: 0.00736961 \n",
      "epoch: 238 :: cost: 0.00230951 :: accuracy: 0.00700299 \n",
      "epoch: 239 :: cost: 0.00351813 :: accuracy: 0.00900189 \n",
      "epoch: 240 :: cost: 0.00297310 :: accuracy: 0.00809314 \n",
      "epoch: 241 :: cost: 0.00322518 :: accuracy: 0.00829981 \n",
      "epoch: 242 :: cost: 0.00297408 :: accuracy: 0.00788924 \n",
      "epoch: 243 :: cost: 0.00306119 :: accuracy: 0.00819247 \n",
      "epoch: 244 :: cost: 0.00278514 :: accuracy: 0.00777273 \n",
      "epoch: 245 :: cost: 0.00313847 :: accuracy: 0.00815685 \n",
      "epoch: 246 :: cost: 0.00268667 :: accuracy: 0.00742527 \n",
      "epoch: 247 :: cost: 0.00341718 :: accuracy: 0.00872140 \n",
      "epoch: 248 :: cost: 0.00323251 :: accuracy: 0.00841246 \n",
      "epoch: 249 :: cost: 0.00263489 :: accuracy: 0.00733295 \n",
      "epoch: 250 :: cost: 0.00226223 :: accuracy: 0.00679998 \n",
      "epoch: 251 :: cost: 0.00365197 :: accuracy: 0.00909977 \n",
      "epoch: 252 :: cost: 0.00342814 :: accuracy: 0.00870943 \n",
      "epoch: 253 :: cost: 0.00084964 :: accuracy: 0.00529320 \n",
      "epoch: 254 :: cost: 0.00333422 :: accuracy: 0.00839900 \n",
      "epoch: 255 :: cost: 0.00335383 :: accuracy: 0.00843823 \n",
      "epoch: 256 :: cost: 0.00104161 :: accuracy: 0.00544034 \n",
      "epoch: 257 :: cost: 0.00317540 :: accuracy: 0.00824650 \n",
      "epoch: 258 :: cost: 0.00332178 :: accuracy: 0.00845571 \n",
      "epoch: 259 :: cost: 0.00150972 :: accuracy: 0.00579122 \n",
      "epoch: 260 :: cost: 0.00280490 :: accuracy: 0.00752746 \n",
      "epoch: 261 :: cost: 0.00283720 :: accuracy: 0.00762184 \n",
      "epoch: 262 :: cost: 0.00190277 :: accuracy: 0.00636106 \n",
      "epoch: 263 :: cost: 0.00230677 :: accuracy: 0.00687214 \n",
      "epoch: 264 :: cost: 0.00182285 :: accuracy: 0.00611648 \n",
      "epoch: 265 :: cost: 0.00224662 :: accuracy: 0.00662568 \n",
      "epoch: 266 :: cost: 0.00175262 :: accuracy: 0.00602792 \n",
      "epoch: 267 :: cost: 0.00199919 :: accuracy: 0.00645312 \n",
      "epoch: 268 :: cost: 0.00209713 :: accuracy: 0.00656416 \n",
      "epoch: 269 :: cost: 0.00125403 :: accuracy: 0.00545422 \n",
      "epoch: 270 :: cost: 0.00158180 :: accuracy: 0.00576064 \n",
      "epoch: 271 :: cost: 0.00129340 :: accuracy: 0.00552970 \n",
      "epoch: 272 :: cost: 0.00123143 :: accuracy: 0.00548058 \n",
      "epoch: 273 :: cost: 0.00145760 :: accuracy: 0.00562176 \n",
      "epoch: 274 :: cost: 0.00116130 :: accuracy: 0.00532647 \n",
      "epoch: 275 :: cost: 0.00155681 :: accuracy: 0.00579226 \n",
      "epoch: 276 :: cost: 0.00120072 :: accuracy: 0.00538197 \n",
      "epoch: 277 :: cost: 0.00155940 :: accuracy: 0.00567723 \n",
      "epoch: 278 :: cost: 0.00108028 :: accuracy: 0.00521796 \n",
      "epoch: 279 :: cost: 0.00151864 :: accuracy: 0.00570977 \n",
      "epoch: 280 :: cost: 0.00098752 :: accuracy: 0.00513926 \n",
      "epoch: 281 :: cost: 0.00126186 :: accuracy: 0.00531510 \n",
      "epoch: 282 :: cost: 0.00117968 :: accuracy: 0.00528329 \n",
      "epoch: 283 :: cost: 0.00086276 :: accuracy: 0.00502359 \n",
      "epoch: 284 :: cost: 0.00151065 :: accuracy: 0.00555854 \n",
      "epoch: 285 :: cost: 0.00088832 :: accuracy: 0.00497013 \n",
      "epoch: 286 :: cost: 0.00149159 :: accuracy: 0.00558029 \n",
      "epoch: 287 :: cost: 0.00105428 :: accuracy: 0.00509887 \n",
      "epoch: 288 :: cost: 0.00118726 :: accuracy: 0.00517704 \n",
      "epoch: 289 :: cost: 0.00132054 :: accuracy: 0.00533769 \n",
      "epoch: 290 :: cost: 0.00089764 :: accuracy: 0.00491868 \n",
      "epoch: 291 :: cost: 0.00165631 :: accuracy: 0.00564751 \n",
      "epoch: 292 :: cost: 0.00109817 :: accuracy: 0.00507797 \n",
      "epoch: 293 :: cost: 0.00173307 :: accuracy: 0.00578846 \n",
      "epoch: 294 :: cost: 0.00121303 :: accuracy: 0.00513036 \n",
      "epoch: 295 :: cost: 0.00184668 :: accuracy: 0.00581998 \n",
      "epoch: 296 :: cost: 0.00115251 :: accuracy: 0.00505985 \n",
      "epoch: 297 :: cost: 0.00223783 :: accuracy: 0.00646496 \n",
      "epoch: 298 :: cost: 0.00182116 :: accuracy: 0.00585356 \n",
      "epoch: 299 :: cost: 0.00179781 :: accuracy: 0.00571083 \n",
      "epoch: 300 :: cost: 0.00183681 :: accuracy: 0.00575394 \n",
      "[[48  0  1  1]\n",
      " [ 1 49  0  0]\n",
      " [ 0  0 50  0]\n",
      " [ 0  1  0 49]]\n",
      "Total Data Points:  10000\n",
      "Total Missing Points:  500\n",
      "Accuracy: 99.42\n",
      "Cluster Accuracy: 98.00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import sklearn.metrics\n",
    "import sklearn.cluster\n",
    "from munkres import Munkres\n",
    "import scipy.io as sio\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0: all messages are logged (default), 1: INFO messages are not printed, 2: INFO and WARNING messages are not printed, 3: ERROR messages are not printed\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "class ConvAE(object):\n",
    "\tdef __init__(self, input_shape,flat_layer_size,auto_layer_size,deco_layer_size,  reg_const1 = 1.0, reg_const2 = 1.0, reg = None, batch_size = 200,\\\n",
    "\t\t model_path = None, logs_path = 'E:\\MS\\Semester\\Summer\\RA\\Synthetic Data\\logs'):\t\n",
    "\n",
    "\t\tself.input_shape = input_shape\n",
    "\t\tself.n_features = input_shape[1]\n",
    "\t\tself.model_path = model_path\t\t\t\n",
    "\t\tself.iter = 0\n",
    "\t\tself.batch_size = batch_size\n",
    "\n",
    "\t\tself.flat_layer_size = flat_layer_size\n",
    "\t\tself.auto_layer_size = auto_layer_size\n",
    "\t\tself.deco_layer_size = deco_layer_size\n",
    "\n",
    "\t\t# model\n",
    "\t\tself.x = tf.compat.v1.placeholder(tf.float32, [None, self.n_features],name= \"Input_data\")\n",
    "\t\tself.learning_rate = tf.compat.v1.placeholder(tf.float32, [], name = \"learning_rate\")\n",
    "\t\t\n",
    "\n",
    "\t\tself.encoded = self.encoder(self.x)\n",
    "\t\tself.z_ssc, Coef = self.selfexpressive_module()\t\n",
    "\t\tself.Coef = Coef\t\t\t\t\t\t\n",
    "\t\tself.decoded = self.decoder(self.z_ssc)\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tself.saver = tf.compat.v1.train.Saver()\n",
    "\t\t\n",
    "\t\twith tf.name_scope(\"Loss\") as scope:\n",
    "\t\t\t\n",
    "\t\t\t# Loss with X_omega and X_omega_hat\n",
    "\t\t\tself.x_omega, self.mask_tensor = convert_nan(self.x)\n",
    "\t\t\tself.x_omega_hat = self.decoded * self.mask_tensor\n",
    "\t\t\tself.reconstruction_cost =  0.5*tf.norm(tf.subtract(self.x_omega_hat, self.x_omega), ord='fro',axis=(0,1))\n",
    "\n",
    "\t\t\t# Loss with full data\n",
    "\t\t\t# self.reconstruction_cost =  0.5*tf.norm(tf.subtract(self.decoded, self.x),ord='fro',axis=(0,1))\n",
    "\t\t\tself.latent_cost = 0.5 * tf.norm(tf.subtract(self.encoded, self.z_ssc),ord='fro',axis=(0,1))\n",
    "\t\t\tself.coefficient_ssc = tf.norm(self.Coef,ord=2,axis=(0,1))\t\t\t\n",
    "\t\t\tself.loss_ssc = self.latent_cost * reg_const2 + reg_const1 * self.coefficient_ssc + self.reconstruction_cost\n",
    "\t\t\t# self.loss_ssc = reg_const1 * self.coefficient_ssc + self.reconstruction_cost\n",
    "\n",
    "\t\t\t# Adding the losses to logs\n",
    "\t\t\ttf.compat.v1.summary.scalar(\"Self_expressive_Loss\", self.latent_cost)\n",
    "\t\t\ttf.compat.v1.summary.scalar(\"Coefficient_Loss\", self.coefficient_ssc)\n",
    "\t\t\ttf.compat.v1.summary.scalar(\"Reconstruction_Loss\", self.reconstruction_cost) \n",
    "\t\t\ttf.compat.v1.summary.scalar(\"Total_Loss\", self.loss_ssc) \n",
    "\n",
    "\t\t# autoencoder_variables = tf.compat.v1.trainable_variables()\n",
    "\t\t# for var in autoencoder_variables:\n",
    "\t\t# \ttf.compat.v1.summary.histogram(var.name, var)\n",
    "\t\t\t\n",
    "\t\tself.merged_summary_op = tf.compat.v1.summary.merge_all()\t\t\n",
    "\t\tself.optimizer_ssc = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate).minimize(self.loss_ssc)\n",
    "\n",
    "\t\tself.init = tf.compat.v1.global_variables_initializer()\n",
    "\t\tself.sess = tf.compat.v1.InteractiveSession()\n",
    "\t\tself.sess.run(self.init)\n",
    "\t\tfor i, var in enumerate(self.saver._var_list):\n",
    "\t\t\tprint('Var {}: {}'.format(i, var))\n",
    "\t\tself.summary_writer = tf.compat.v1.summary.FileWriter(logs_path, graph=tf.compat.v1.get_default_graph())\n",
    "\n",
    "\t# Building the encoder\n",
    "\t# Building the encoder\n",
    "\tdef encoder(self,x):\n",
    "\n",
    "\t\twith tf.name_scope(\"Flat_Flayer\") as scope:\n",
    "\t\t\t    \n",
    "\t\t\tstacked_tensors_list = []\n",
    "\t\t\tfor i in range(0,self.input_shape[1]):\n",
    "\t\t\t\tinput_flat = tf.reshape(x[:,i],[1,self.flat_layer_size[0]])\n",
    "\t\t\t\tinput_missing_flat , mask_flat_tensor = convert_nan(input_flat)\n",
    "\t\t\t\tflat_layer_output = tf.compat.v1.layers.dense(input_missing_flat, self.input_shape[0], activation=tf.nn.relu, name=\"flattened_layer_\"+str(i + 1),kernel_initializer=None,bias_initializer=tf.compat.v1.zeros_initializer())\n",
    "\n",
    "\t\t\t\tstacked_tensors_list.append(tf.transpose(flat_layer_output))\n",
    "\t\t\t\n",
    "\t\t\tstacked_flat_tensor = tf.concat(stacked_tensors_list, axis=1)\n",
    "\t\n",
    "\t\twith tf.name_scope(\"Encoder\") as scope:\n",
    "\t\t\tencoder_layer = stacked_flat_tensor\n",
    "\t\t\tfor i in range(0,len(self.auto_layer_size)):\n",
    "\t\t\t\tencoder_layer = tf.compat.v1.layers.dense(encoder_layer, self.auto_layer_size[i], activation=tf.nn.relu, name=\"encoder_layer_\"+str(i + 1),kernel_initializer=None,bias_initializer=tf.compat.v1.zeros_initializer())\n",
    "\n",
    "\t\t\treturn  encoder_layer\n",
    "\n",
    "\t# Building the decoder\n",
    "\tdef decoder(self,z):\n",
    "\t\twith tf.name_scope(\"Decoder\") as scope:\n",
    "\t\t\tdecoder_layer = z\n",
    "\n",
    "\t\t\tfor i in range(0,len(self.deco_layer_size)):\n",
    "\t\t\t\tdecoder_layer = tf.compat.v1.layers.dense(decoder_layer, self.deco_layer_size[i], activation=tf.nn.relu, name=\"decoder_layer_\"+str(i + 1),kernel_initializer=None,bias_initializer=tf.compat.v1.zeros_initializer())\n",
    "\n",
    "\t\treturn decoder_layer\n",
    "\n",
    "\t#Building the Self-Expressive Layer\n",
    "\tdef selfexpressive_module(self):\n",
    "\t\twith tf.name_scope(\"Self-Expressive_Layer\") as scope:\n",
    "\t\t\tCoef = tf.Variable(1.0e-4 * tf.ones([self.batch_size, self.batch_size],tf.float32), name = 'Coef')\t\t\t\n",
    "\t\t\tz_ssc = tf.matmul(Coef,\tself.encoded)\n",
    "\n",
    "\t\t\treturn z_ssc, Coef\n",
    "\n",
    "\n",
    "\tdef finetune_fit(self, X, lr):\n",
    "\t\tC, cost, complete_data, summary, _ = self.sess.run((self.Coef, self.reconstruction_cost, self.decoded , self.merged_summary_op, self.optimizer_ssc), feed_dict = {self.x: X, self.learning_rate: lr})\n",
    "\t\tself.summary_writer.add_summary(summary, self.iter)\n",
    "\t\tself.iter = self.iter + 1\n",
    "\t\treturn C, cost, complete_data\n",
    "\t\n",
    "\tdef initlization(self):\n",
    "\t\tself.sess.run(self.init)\t\n",
    "def best_map(L1,L2):\n",
    "\t#L1 should be the labels and L2 should be the clustering number we got\n",
    "\tLabel1 = np.unique(L1)\n",
    "\tnClass1 = len(Label1)\n",
    "\tLabel2 = np.unique(L2)\n",
    "\tnClass2 = len(Label2)\n",
    "\tnClass = np.maximum(nClass1,nClass2)\n",
    "\tG = np.zeros((nClass,nClass))\n",
    "\tfor i in range(nClass1):\n",
    "\t\tind_cla1 = L1 == Label1[i]\n",
    "\t\tind_cla1 = ind_cla1.astype(float)\n",
    "\t\tfor j in range(nClass2):\n",
    "\t\t\tind_cla2 = L2 == Label2[j]\n",
    "\t\t\tind_cla2 = ind_cla2.astype(float)\n",
    "\t\t\tG[i,j] = np.sum(ind_cla2 * ind_cla1)\n",
    "\tm = Munkres()\n",
    "\tindex = m.compute(-G.T)\n",
    "\tindex = np.array(index)\n",
    "\tc = index[:,1]\n",
    "\tnewL2 = np.zeros(L2.shape)\n",
    "\tfor i in range(nClass2):\n",
    "\t\tnewL2[L2 == Label2[i]] = Label1[c[i]]\n",
    "\treturn newL2\n",
    "\n",
    "def thrC(C,ro):\n",
    "\tif ro < 1:\n",
    "\t\tN = C.shape[1]\n",
    "\t\tCp = np.zeros((N,N))\n",
    "\t\tS = np.abs(np.sort(-np.abs(C),axis=0))\n",
    "\t\tInd = np.argsort(-np.abs(C),axis=0)\n",
    "\t\tfor i in range(N):\n",
    "\t\t\tcL1 = np.sum(S[:,i]).astype(float)\n",
    "\t\t\tstop = False\n",
    "\t\t\tcsum = 0\n",
    "\t\t\tt = 0\n",
    "\t\t\twhile(stop == False):\n",
    "\t\t\t\tcsum = csum + S[t,i]\n",
    "\t\t\t\tif csum > ro*cL1:\n",
    "\t\t\t\t\tstop = True\n",
    "\t\t\t\t\tCp[Ind[0:t+1,i],i] = C[Ind[0:t+1,i],i]\n",
    "\t\t\t\tt = t + 1\n",
    "\telse:\n",
    "\t\tCp = C\n",
    "\n",
    "\treturn Cp\n",
    "\n",
    "def post_proC(C, K, d, alpha):\n",
    "\t# C: coefficient matrix, K: number of clusters, d: dimension of each subspace\n",
    "\tC = 0.5*(C + C.T)\n",
    "\tr = d*K + 1\t\n",
    "\tU, S, _ = svds(C,r,v0 = np.ones(C.shape[0]))\n",
    "\tU = U[:,::-1] \n",
    "\tS = np.sqrt(S[::-1])\n",
    "\tS = np.diag(S)\n",
    "\tU = U.dot(S)\n",
    "\tU = normalize(U, norm='l2', axis = 1)  \n",
    "\tZ = U.dot(U.T)\n",
    "\tZ = Z * (Z>0)\n",
    "\tL = np.abs(Z ** alpha)\n",
    "\tL = L/L.max()\n",
    "\tL = 0.5 * (L + L.T)\t\n",
    "\tspectral = sklearn.cluster.SpectralClustering(n_clusters=K, eigen_solver='arpack', affinity='precomputed',assign_labels='discretize')\n",
    "\tspectral.fit(L)\n",
    "\tgrp = spectral.fit_predict(L) + 1 \n",
    "\treturn grp, L\n",
    "\n",
    "def err_rate(gt_s, s):\n",
    "\tc_x = best_map(gt_s,s)\n",
    "\terr_x = np.sum(gt_s[:] != c_x[:])\n",
    "\tmissrate = err_x.astype(float) / (gt_s.shape[0])\n",
    "\tprint(sklearn.metrics.confusion_matrix(gt_s, c_x))\n",
    "\treturn missrate  \n",
    "\n",
    "def convert_nan(input): # Function to create a Mask tensor with 1s in available data and zero in missing data\n",
    "\tnan_mask = tf.math.is_nan(input)\n",
    "\tx_omega = tf.where(nan_mask, tf.zeros_like(input), input)\n",
    "\tmask_tensor = tf.where(nan_mask, tf.zeros_like(input), tf.ones_like(input))\n",
    "\treturn x_omega, mask_tensor\n",
    "\t\n",
    "def missing_data_generation(input,num_nans): # Function to create random Nan values \n",
    "\t# num_nans: Choose the desired number of NaN values\n",
    "\tindices = np.random.choice(input.size, size=num_nans, replace=False)\n",
    "\tinput.ravel()[indices] = np.nan\n",
    "\treturn input\n",
    "\n",
    "# main function starts here\n",
    "if __name__ == '__main__':\n",
    "\t\t\n",
    "\twith open('E:\\MS\\Semester\\Summer\\RA\\Synthetic Data\\syndata200x50.csv', 'r') as f: \n",
    "\t\tdata = np.genfromtxt(f, dtype='f4', delimiter=',')\n",
    "\n",
    "\tfull_data = copy.deepcopy(data) # Deep copy of full data\n",
    "\tinput_shape = np.shape(data)\n",
    "\tbatch_size = input_shape[0] # Total number of Samples\n",
    "\n",
    "\t# Model Architecture\n",
    "\tflat_layer_size = [input_shape[0]] # Flat Layer Neurons Size\n",
    "\tauto_layer_size = [40] # Encoder Layer Neurons Size (Add multiple for more layers)\n",
    "\tdeco_layer_size = [50] # Decoder Layer Neurons Size (Add multiple for more layers)\n",
    "\n",
    "\tlogs_path = 'E:\\MS\\Semester\\Summer\\RA\\Synthetic Data\\logs'\n",
    "\n",
    "\t# Learning Parameters\n",
    "\tepoch = 300\n",
    "\n",
    "\t#Learning Rate\n",
    "\tstart_value = 1e-2 \n",
    "\tend_value = 0.5e-3\n",
    "\tnum_points = 6\n",
    "\tlearning_rate_skip = int(epoch/num_points)\n",
    "\texponential_learning_rate = np.geomspace(start_value, end_value, num_points)\n",
    "\n",
    "\tnum_class = 4\n",
    "\treg1 = 1.0 # regularization constant for Coefficient Loss\n",
    "\treg2 = 0.01 # regularization constant for Self-Expressive Loss\n",
    "\talpha = 1 #max(0.4 - (num_class-1)/10 * 0.1, 0.1)\n",
    "\n",
    "\t\t\n",
    "\tlabels = []\n",
    "\tfor j in range(1,5):\n",
    "\t\tperc_num_list = [j for i in range(0,50)] \n",
    "\t\tlabels.append(perc_num_list)\n",
    "\tlabels = np.array(labels)\n",
    "\ttrue_labels = labels.flatten()\n",
    "\n",
    "\t\n",
    "\tmissing_data_num = 500 # Choose the desired number of NaN values\n",
    "\tmissing_data_input = missing_data_generation(data,missing_data_num)\n",
    "\n",
    "\ttf.compat.v1.reset_default_graph()\n",
    "\tCAE = ConvAE( input_shape, flat_layer_size,auto_layer_size,deco_layer_size, reg_const1 = reg1, reg_const2 = reg2, reg = None, batch_size = 200,\\\n",
    "\t\t\tmodel_path = None, logs_path = logs_path)\n",
    "\n",
    "\tdata_norm = tf.norm(full_data,ord='fro',axis=(0,1)).eval() # Frobenius norm of full data\n",
    "\n",
    "\tCAE.initlization()\n",
    "\tfor iter_ft  in range(epoch):\n",
    "\t\tif (iter_ft % learning_rate_skip == 0):\n",
    "\t\t\tlearning_rate = exponential_learning_rate[int(iter_ft/learning_rate_skip)]\n",
    "\t\tC, cost, complete_data = CAE.finetune_fit(missing_data_input,learning_rate)\n",
    "\t\taccuracy = tf.norm(tf.subtract(complete_data, full_data),ord='fro',axis=(0,1)).eval()/data_norm\n",
    "\t\tprint (\"epoch: %.1d ::\" % (iter_ft+1), \"cost: %.8f ::\" %  (cost/data_norm), \"accuracy: %.8f \" % (accuracy))\n",
    "\t\tif ((iter_ft+1) % 300 == 0): # and (iter_ft >= 50):\n",
    "\t\t\tC = thrC(C,alpha)\t\t\t\n",
    "\t\t\ty_x, CKSym_x = post_proC(C, num_class, 3 , 4)\t\t\t\n",
    "\t\t\tmissrate_x = err_rate(true_labels,y_x)\t\t\t\n",
    "\t\t\tcluster_acc = 1 - missrate_x\n",
    "\t\t\t\n",
    "\n",
    "\t\n",
    "\tprint(\"Total Data Points: \", input_shape[0]*input_shape[1])\n",
    "\tprint(\"Total Missing Points: \", missing_data_num)\n",
    "\tprint(\"Accuracy: %.2f\" % ((1 - accuracy)*100))\n",
    "\tprint(\"Cluster Accuracy: %.2f\" % (cluster_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
