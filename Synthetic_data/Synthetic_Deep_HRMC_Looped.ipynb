{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ji, P., Zhang, T., Li, H., Salzmann, M., Reid, I. (2017). Deep subspace clustering networks.\n",
    "# Advances in neural information processing systems, 30.\n",
    "# The following code has been modified and adapted from the above paper.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import sklearn.cluster\n",
    "from munkres import Munkres\n",
    "import scipy.io as sio\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0: all messages are logged (default), 1: INFO messages are not printed, 2: INFO and WARNING messages are not printed, 3: ERROR messages are not printed\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "def train_Deep_HRMC_network(missing_data_num): # Main function to perform missing entry prediction for a certain number of values\n",
    "\tclass ConvAE(object):\n",
    "\t\tdef __init__(self, input_shape,flat_layer_size,auto_layer_size,deco_layer_size,  reg_const1 = 1.0, reg_const2 = 1.0, reg = None, batch_size = 200,\\\n",
    "\t\t\tmodel_path = None, logs_path = 'E:\\MS\\Semester\\Summer\\RA\\Synthetic Data\\logs'):\t\n",
    "\n",
    "\t\t\tself.input_shape = input_shape\n",
    "\t\t\tself.n_features = input_shape[1]\n",
    "\t\t\tself.model_path = model_path\t\t\t\n",
    "\t\t\tself.iter = 0\n",
    "\t\t\tself.batch_size = batch_size\n",
    "\n",
    "\t\t\tself.flat_layer_size = flat_layer_size\n",
    "\t\t\tself.auto_layer_size = auto_layer_size\n",
    "\t\t\tself.deco_layer_size = deco_layer_size\n",
    "\n",
    "\t\t\t# model\n",
    "\t\t\tself.x = tf.compat.v1.placeholder(tf.float32, [None, self.n_features],name= \"Input_data\")\n",
    "\t\t\tself.learning_rate = tf.compat.v1.placeholder(tf.float32, [], name = \"learning_rate\")\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tself.encoded = self.encoder(self.x)\n",
    "\t\t\tself.z_ssc, Coef = self.selfexpressive_module()\t\n",
    "\t\t\tself.Coef = Coef\t\t\t\t\t\t\n",
    "\t\t\tself.decoded = self.decoder(self.z_ssc)\t\t\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tself.saver = tf.compat.v1.train.Saver()\n",
    "\t\t\t\n",
    "\t\t\twith tf.name_scope(\"Loss\") as scope:\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Loss with X_omega and X_omega_hat\n",
    "\t\t\t\tself.x_omega, self.mask_tensor = convert_nan(self.x)\n",
    "\t\t\t\tself.x_omega_hat = self.decoded * self.mask_tensor\n",
    "\t\t\t\tself.reconstruction_cost =  0.5*tf.norm(tf.subtract(self.x_omega_hat, self.x_omega), ord='fro',axis=(0,1))\n",
    "\n",
    "\t\t\t\t# Loss with full data\n",
    "\t\t\t\t# self.reconstruction_cost =  0.5*tf.norm(tf.subtract(self.decoded, self.x),ord='fro',axis=(0,1))\n",
    "\t\t\t\tself.latent_cost = 0.5 * tf.norm(tf.subtract(self.encoded, self.z_ssc),ord='fro',axis=(0,1))\n",
    "\t\t\t\tself.coefficient_ssc = tf.norm(self.Coef,ord=2,axis=(0,1))\t\t\t\n",
    "\t\t\t\tself.loss_ssc = self.latent_cost * reg_const2 + reg_const1 * self.coefficient_ssc + self.reconstruction_cost\n",
    "\t\t\t\t# self.loss_ssc = reg_const1 * self.coefficient_ssc + self.reconstruction_cost\n",
    "\n",
    "\t\t\t\t# Adding the losses to logs\n",
    "\t\t\t\ttf.compat.v1.summary.scalar(\"Self_expressive_Loss\", self.latent_cost)\n",
    "\t\t\t\ttf.compat.v1.summary.scalar(\"Coefficient_Loss\", self.coefficient_ssc)\n",
    "\t\t\t\ttf.compat.v1.summary.scalar(\"Reconstruction_Loss\", self.reconstruction_cost) \n",
    "\t\t\t\ttf.compat.v1.summary.scalar(\"Total_Loss\", self.loss_ssc) \n",
    "\n",
    "\t\t\tautoencoder_variables = tf.compat.v1.trainable_variables()\n",
    "\t\t\tfor var in autoencoder_variables:\n",
    "\t\t\t\ttf.compat.v1.summary.histogram(var.name, var)\n",
    "\t\t\t\t\n",
    "\t\t\tself.merged_summary_op = tf.compat.v1.summary.merge_all()\t\t\n",
    "\t\t\tself.optimizer_ssc = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate).minimize(self.loss_ssc)\n",
    "\n",
    "\t\t\tself.init = tf.compat.v1.global_variables_initializer()\n",
    "\t\t\tself.sess = tf.compat.v1.InteractiveSession()\n",
    "\t\t\tself.sess.run(self.init)\n",
    "\t\t\tself.summary_writer = tf.compat.v1.summary.FileWriter(logs_path, graph=tf.compat.v1.get_default_graph())\n",
    "\n",
    "\t\t# Building the encoder\n",
    "\t\t# Building the encoder\n",
    "\t\tdef encoder(self,x):\n",
    "\n",
    "\t\t\twith tf.name_scope(\"Flat_Flayer\") as scope:\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tstacked_tensors_list = []\n",
    "\t\t\t\tfor i in range(0,self.input_shape[1]):\n",
    "\t\t\t\t\tinput_flat = tf.reshape(x[:,i],[1,self.flat_layer_size[0]])\n",
    "\t\t\t\t\tinput_missing_flat , mask_flat_tensor = convert_nan(input_flat)\n",
    "\t\t\t\t\tflat_layer_output = tf.compat.v1.layers.dense(input_missing_flat, self.input_shape[0], activation=tf.nn.relu, name=\"flattened_layer_\"+str(i + 1),kernel_initializer=None,bias_initializer=tf.compat.v1.zeros_initializer())\n",
    "\n",
    "\t\t\t\t\tstacked_tensors_list.append(tf.transpose(flat_layer_output))\n",
    "\t\t\t\t\n",
    "\t\t\t\tstacked_flat_tensor = tf.concat(stacked_tensors_list, axis=1)\n",
    "\t\t\n",
    "\t\t\twith tf.name_scope(\"Encoder\") as scope:\n",
    "\t\t\t\tencoder_layer = stacked_flat_tensor\n",
    "\t\t\t\tfor i in range(0,len(self.auto_layer_size)):\n",
    "\t\t\t\t\tencoder_layer = tf.compat.v1.layers.dense(encoder_layer, self.auto_layer_size[i], activation=tf.nn.relu, name=\"encoder_layer_\"+str(i + 1),kernel_initializer=None,bias_initializer=tf.compat.v1.zeros_initializer())\n",
    "\n",
    "\t\t\t\treturn  encoder_layer\n",
    "\n",
    "\t\t# Building the decoder\n",
    "\t\tdef decoder(self,z):\n",
    "\t\t\twith tf.name_scope(\"Decoder\") as scope:\n",
    "\t\t\t\tdecoder_layer = z\n",
    "\n",
    "\t\t\t\tfor i in range(0,len(self.deco_layer_size)):\n",
    "\t\t\t\t\tdecoder_layer = tf.compat.v1.layers.dense(decoder_layer, self.deco_layer_size[i], activation=tf.nn.relu, name=\"decoder_layer_\"+str(i + 1),kernel_initializer=None,bias_initializer=tf.compat.v1.zeros_initializer())\n",
    "\n",
    "\t\t\treturn decoder_layer\n",
    "\n",
    "\t\t#Building the Self-Expressive Layer\n",
    "\t\tdef selfexpressive_module(self):\n",
    "\t\t\twith tf.name_scope(\"Self-Expressive_Layer\") as scope:\n",
    "\t\t\t\tCoef = tf.Variable(1.0e-4 * tf.ones([self.batch_size, self.batch_size],tf.float32), name = 'Coef')\t\t\t\n",
    "\t\t\t\tz_ssc = tf.matmul(Coef,\tself.encoded)\n",
    "\n",
    "\t\t\t\treturn z_ssc, Coef\n",
    "\n",
    "\n",
    "\t\tdef finetune_fit(self, X, lr):\n",
    "\t\t\tC, cost, complete_data, summary, _ = self.sess.run((self.Coef, self.reconstruction_cost, self.decoded , self.merged_summary_op, self.optimizer_ssc), feed_dict = {self.x: X, self.learning_rate: lr})\n",
    "\t\t\tself.summary_writer.add_summary(summary, self.iter)\n",
    "\t\t\tself.iter = self.iter + 1\n",
    "\t\t\treturn C, cost, complete_data\n",
    "\t\t\n",
    "\t\tdef initlization(self):\n",
    "\t\t\tself.sess.run(self.init)\t\n",
    "\n",
    "\t\tdef close_session(self):\n",
    "\t\t\tself.sess.close()\n",
    "\n",
    "\tdef best_map(L1,L2):\n",
    "\t#L1 should be the labels and L2 should be the clustering number we got\n",
    "\t\tLabel1 = np.unique(L1)\n",
    "\t\tnClass1 = len(Label1)\n",
    "\t\tLabel2 = np.unique(L2)\n",
    "\t\tnClass2 = len(Label2)\n",
    "\t\tnClass = np.maximum(nClass1,nClass2)\n",
    "\t\tG = np.zeros((nClass,nClass))\n",
    "\t\tfor i in range(nClass1):\n",
    "\t\t\tind_cla1 = L1 == Label1[i]\n",
    "\t\t\tind_cla1 = ind_cla1.astype(float)\n",
    "\t\t\tfor j in range(nClass2):\n",
    "\t\t\t\tind_cla2 = L2 == Label2[j]\n",
    "\t\t\t\tind_cla2 = ind_cla2.astype(float)\n",
    "\t\t\t\tG[i,j] = np.sum(ind_cla2 * ind_cla1)\n",
    "\n",
    "\t\tm = Munkres()\n",
    "\t\tindex = m.compute(-G.T)\n",
    "\t\tindex = np.array(index)\n",
    "\t\tc = index[:,1]\n",
    "\t\tnewL2 = np.zeros(L2.shape)\n",
    "\t\tfor i in range(nClass2):\n",
    "\t\t\tnewL2[L2 == Label2[i]] = Label1[c[i]]\n",
    "\t\treturn newL2\n",
    "\n",
    "\tdef post_proC(C, K, d, alpha):\n",
    "\t\t# C: coefficient matrix, K: number of clusters, d: dimension of each subspace\n",
    "\t\tC = 0.5*(C + C.T)\n",
    "\t\tr = d*K + 1\t\n",
    "\t\tU, S, _ = svds(C,r,v0 = np.ones(C.shape[0]))\n",
    "\t\tU = U[:,::-1] \n",
    "\t\tS = np.sqrt(S[::-1])\n",
    "\t\tS = np.diag(S)\n",
    "\t\tU = U.dot(S)\n",
    "\t\tU = normalize(U, norm='l2', axis = 1)  \n",
    "\t\tZ = U.dot(U.T)\n",
    "\t\tZ = Z * (Z>0)\n",
    "\t\tL = np.abs(Z ** alpha)\n",
    "\t\tL = L/L.max()\n",
    "\t\tL = 0.5 * (L + L.T)\t\n",
    "\t\tspectral = sklearn.cluster.SpectralClustering(n_clusters=K, eigen_solver='arpack', affinity='precomputed',assign_labels='discretize')\n",
    "\t\tspectral.fit(L)\n",
    "\t\tgrp = spectral.fit_predict(L) + 1 \n",
    "\t\treturn grp, L\n",
    "\n",
    "\tdef err_rate(gt_s, s):\n",
    "\t\tc_x = best_map(gt_s,s)\n",
    "\t\terr_x = np.sum(gt_s[:] != c_x[:])\n",
    "\t\tmissrate = err_x.astype(float) / (gt_s.shape[0])\n",
    "\t\treturn missrate \n",
    "\n",
    "\t# main function starts here\n",
    "\tif __name__ == '__main__':\n",
    "\t\t\t\n",
    "\t\twith open('E:\\MS\\Semester\\Summer\\RA\\Synthetic Data\\syndata200x50.csv', 'r') as f: \n",
    "\t\t\tdata = np.genfromtxt(f, dtype='f4', delimiter=',')\n",
    "\n",
    "\t\tfull_data = copy.deepcopy(data) # Deep copy of full data\n",
    "\t\tinput_shape = np.shape(data)\n",
    "\t\tbatch_size = input_shape[0] # Total number of Samples\n",
    "\n",
    "\t\t# Model Architecture\n",
    "\t\tflat_layer_size = [input_shape[0]] # Flat Layer Neurons Size\n",
    "\t\tauto_layer_size = [40] # Encoder Layer Neurons Size (Add multiple for more layers)\n",
    "\t\tdeco_layer_size = [50] # Decoder Layer Neurons Size (Add multiple for more layers)\n",
    "\n",
    "\t\tlogs_path = 'E:\\MS\\Semester\\Summer\\RA\\Synthetic Data\\logs'\n",
    "\n",
    "\t\t# Learning Parameters\n",
    "\t\tepoch = 300\n",
    "\n",
    "\t\t#Learning Rate\n",
    "\t\tstart_value = 1e-2 \n",
    "\t\tend_value = 0.5e-3\n",
    "\t\tnum_points = 6\n",
    "\t\tlearning_rate_skip = int(epoch/num_points)\n",
    "\t\texponential_learning_rate = np.geomspace(start_value, end_value, num_points)\n",
    "\n",
    "\t\treg1 = 1.0 # regularization constant for Coefficient Loss\n",
    "\t\treg2 = 0.01 # regularization constant for Self-Expressive Loss\n",
    "\t\tnum_class = 4 \n",
    "\n",
    "\t\t\n",
    "\t\tlabels = []\n",
    "\t\tfor j in range(1,5):\n",
    "\t\t\tperc_num_list = [j for i in range(0,50)] \n",
    "\t\t\tlabels.append(perc_num_list)\n",
    "\t\tlabels = np.array(labels)\n",
    "\t\ttrue_labels = labels.flatten()\n",
    "\n",
    "\t\tdef convert_nan(input): # Function to create a Mask tensor with 1s in available data and zero in missing data\n",
    "\t\t\tnan_mask = tf.math.is_nan(input)\n",
    "\t\t\tx_omega = tf.where(nan_mask, tf.zeros_like(input), input)\n",
    "\t\t\tmask_tensor = tf.where(nan_mask, tf.zeros_like(input), tf.ones_like(input))\n",
    "\t\t\treturn x_omega, mask_tensor\n",
    "\t\t\n",
    "\t\tdef convert_nan_reverse(input): # Function to create a Mask tensor with 1s in missing data and zero in available data \n",
    "\t\t\tnan_mask = tf.math.is_nan(input)\n",
    "\t\t\tx_omega = tf.where(nan_mask, tf.zeros_like(input), input)\n",
    "\t\t\tmask_tensor = tf.where(nan_mask, tf.ones_like(input),tf.zeros_like(input))\n",
    "\t\t\treturn x_omega, mask_tensor\n",
    "\n",
    "\t\tdef missing_data_generation(input,num_nans): # Function to create random Nan values \n",
    "\t\t\t# num_nans: Choose the desired number of NaN values\n",
    "\t\t\tindices = np.random.choice(input.size, size=num_nans, replace=False)\n",
    "\t\t\tinput.ravel()[indices] = np.nan\n",
    "\t\t\treturn input\n",
    "\n",
    "\t\ttf.compat.v1.reset_default_graph()\n",
    "\t\tmissing_data_input = missing_data_generation(data,missing_data_num)\n",
    "\t\tCAE = ConvAE( input_shape, flat_layer_size,auto_layer_size,deco_layer_size, reg_const1 = reg1, reg_const2 = reg2, reg = None, batch_size = 200,\\\n",
    "\t\t\t\tmodel_path = None, logs_path = logs_path)\n",
    "\n",
    "\t\tdata_norm = tf.norm(full_data,ord='fro',axis=(0,1)).eval() # Frobenius norm of full data\n",
    "\t\tCAE.initlization()\n",
    "\t\tfor iter_ft  in range(epoch):\n",
    "\t\t\tif (iter_ft % learning_rate_skip == 0):\n",
    "\t\t\t\tlearning_rate = exponential_learning_rate[int(iter_ft/learning_rate_skip)]\n",
    "\t\t\tC, cost, complete_data = CAE.finetune_fit(missing_data_input,learning_rate)\n",
    "\t\t\taccuracy = tf.norm(tf.subtract(complete_data, full_data),ord='fro',axis=(0,1)).eval()/data_norm\n",
    "\t\t\tif ((iter_ft+1) % 300 == 0):\t\t\t\n",
    "\t\t\t\ty_hat, _ = post_proC(C, num_class, 3 , 4)\t\t\t\n",
    "\t\t\t\tmissrate_x = err_rate(true_labels,y_hat)\t\t\t\n",
    "\t\t\t\tcluster_acc = 1 - missrate_x\n",
    "\t\tCAE.close_session()\n",
    "\n",
    "\t\treturn (1-accuracy)*100, cluster_acc*100\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_num_list = [i for i in range(0,10000,500)] # Creating a list of missing numbers in inceremebts of 500\n",
    "accuracy_list = []\n",
    "cluster_accuracy_list = []\n",
    "for missing_num in missing_num_list:\n",
    "\taccuracy, cluster_accuracy = train_Deep_HRMC_network(missing_num) # Running the above Deep_HRMC_netowrk for different missing values\n",
    "\taccuracy_list.append(accuracy)\n",
    "\tcluster_accuracy_list.append(cluster_accuracy)\n",
    "\tprint (\"Number of missing Points: %.1d ::\" % missing_num, \"Accuracy: %.2f ::\" %  accuracy, \"Cluster Accuracy: %.2f\" % cluster_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot the completion and clustering accuracy for different missing entry percentages \n",
    "perc_num_list = [int(i/100) for i in range(0,10000,500)]\n",
    "plt.plot(perc_num_list, accuracy_list, c='r', label = \"Completion Accuracy\")\n",
    "plt.plot(perc_num_list, cluster_accuracy_list, c='b', label = 'Clustering Accuracy')\n",
    "plt.legend([\"Completion accuracy\", \"Clustering Accuracy\"])\n",
    "plt.title(\"Deep HRMC: Accuracy vs Missing Points on Synthetic Data (200x50)\")\n",
    "plt.xlabel('Percentage of Missing Points')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0,110)\n",
    "plt.xlim(0,110)\n",
    "plt.yticks(range(0, 101, 10))\n",
    "plt.xticks(range(0, 101, 10))\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
